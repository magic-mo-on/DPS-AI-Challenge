Step 1: Install Vertex SDK
pip3 install google-cloud-aiplatform --upgrade --user

Step 2: Create model and deploy endpoint
Next we'll create a Python file and use the SDK to create a model resource and deploy it to an endpoint. From the File editor in Cloud Shell, select File and then New File.
Name the file deploy.py. Open this file in your editor and copy the following code:

from google.cloud import aiplatform

# Create a model resource from public model assets
model = aiplatform.Model.upload(
    display_name="mpg-imported",
    artifact_uri="gs://io-vertex-codelab/mpg-model/",
    serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-3:latest"
)

# Deploy the above model to an endpoint
endpoint = model.deploy(
    machine_type="n1-standard-4"
)

Next, navigate back to the Terminal in Cloud Shell, cd back into your root dir, and run this Python script you've just created:

cd ..
python3 deploy.py | tee deploy-output.txt

You'll see updates logged to your terminal as resources are being created. This will take 10-15 minutes to run. To ensure it's working correctly, navigate to the Models section of your console in Vertex AI:

Click on mgp-imported and you should see your endpoint for that model being created.In your Cloud Shell Terminal, you'll see something like the following log when your endpoint deploy has completed:

Endpoint model deployed. Resource name: projects/your-project-id/locations/us-central1/endpoints/your-endpoint-id

Step 3: Get predictions on the deployed endpoint
In your Cloud Shell editor, create a new file called predict.py

Open predict.py and paste the following code into it:


from google.cloud import aiplatform

endpoint = aiplatform.Endpoint(
    endpoint_name="ENDPOINT_STRING"
)

# A test example we'll send to our model for prediction
test_mpg = [1, 2, 3, 2, -2, -1, -2, -1, 0]

response = endpoint.predict([test_mpg])

print('API response: ', response)

print('Predicted MPG: ', response.predictions[0][0])

Next, go back to your Terminal and enter the following to replace ENDPOINT_STRING in the predict file with your own endpoint:

ENDPOINT=$(cat deploy-output.txt | sed -nre 's:.*Resource name\: (.*):\1:p' | tail -1)
sed -i "s|ENDPOINT_STRING|$ENDPOINT|g" predict.py

Now it's time to run the predict.py file to get a prediction from our deployed model endpoint:

python3 predict.py

